# ALBEF
https://www.bilibili.com/video/BV1uT411q7ef/?spm_id_from=333.337.search-card.all.click&vd_source=0e79f860d037b2f9ea56ed215452c342
在Vision-and-Language Pre-training (VLP) 已有工作中，主要是靠一个预训练的图像检测器来提取图像特征，并用多模态的encoder编码器将图像特征和文本特征融合，然后使用“完形填空、图文匹配”等下游任务来训练多模态encoder。但是这些工作存在着一下问题：用于抽取图像特征和文本特征的模型是分别单独训练的，也就是说他们的特征不在一个空间内，这就是使得多模态encoder不能有效的学习如何去融合两种特征；使用目标检测模型作为图像特征的抽取器是一件很“昂贵”的事情，在预训练阶段，需要标注物体的box位置，在预测阶段需要有高分辨率的图片；目前被广泛使用的图像-文本数据集存在着固有的噪声，这些噪声会影响模型性能。

![image](https://github.com/user-attachments/assets/2294e3bf-5150-4af4-8c7a-549b4fc9733c)
- ITC: image-text contrastive loss: e.g. InfoNCE
- ITM: image-text matching (二分类任务)
- MLM: masked language model
**Momentum Distillation**
  ![image](https://github.com/user-attachments/assets/74c173a0-b89b-4fe5-99d5-bcdea5ea6582)
teacher model，生成soft label

受到MOCO论文的启发，在ITC任务中使用了Momentum Distillation来解决“研究动机”中提出的第三个问题，也就是数据噪声的问题。在预训练阶段，首先会通过moving-average的方式对每个encoder备份；然后会通过备份的encoder计算和当前图像最相似的文本、和当前文本最相似的图片作为“伪标签”；接着将“伪标签”也认为是正确的答案加入到loss里。

加入伪标签之后，ITC的loss也就包含了两个部分，一部分是GroundTruth下的loss，一部分是伪标签下的loss。


# BLIP
**动机**：
- 基于encoder model不适合做generation task
- 基于encoder-decoder model不适合做retrieval task
- 数据噪音大
![image](https://github.com/user-attachments/assets/9f6b5285-dd41-4cf8-88d6-906881f1eb8a)

**CapFilt**
![image](https://github.com/user-attachments/assets/abb11b8e-74c5-44fe-8c0d-785a2e66c544)
生成caption + 过滤掉噪音样本

# BLIP-2
**动机**：预训练太贵了
![image](https://github.com/user-attachments/assets/4a726f01-d139-423b-b0ff-9c21bcde6d63)
冻结的两个大模型，空间区别太大了，所以引入一个小的transformer去matching
stage1:representation learning
![image](https://github.com/user-attachments/assets/e294630e-a2ab-4521-8d1b-f37991939af3)
stage2:生成
![image](https://github.com/user-attachments/assets/d68e6d1a-6065-45b2-be2f-0ad87598044d)



